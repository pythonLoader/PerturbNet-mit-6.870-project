# -*- coding: utf-8 -*-
"""Columnwise_Real_data_Autoencoder_representation_V2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1unHdXy7-Dqer-2RlTZYFt2UrsLFAh1FG

# Loading the libraries
"""

import keras
from keras import layers, regularizers
from keras.optimizers import gradient_descent_v2
from tensorflow.keras.layers import Dropout

from google.colab import drive

import os
import numpy as np

from sklearn.model_selection import train_test_split

"""# Loading and preparing the input data
The input was previously saved as a list of row-wise no
rmalized F matrices in the google drive.
"""

# Mounting google drive to import the data from
drive.mount('/content/gdrive')

# Importing data from google drive
import os
!pwd
os.chdir('gdrive/My Drive/6.8700_Project/Real_Data')

import glob
import pandas as pd

df_list = []
l = 0

print('Rpe data:')
for name in glob.glob('average_K562_essential_normalized_singlecell_combined.csv'):
#for name in glob.glob('average_rpe1_normalized_singlecell_expression_fold_?.csv'):
    print(name)
    df = pd.read_csv(name)
    # removing some rows that had "inf" values for some gene in row 773 and 7003
    df = df.replace([np.inf, -np.inf], np.nan)
    df.dropna(inplace=True)
    df_list.append(df)
    #df_mod = df.pop('gene_id')
    l += 1

print("Data frame \n", df_list[0])
print("Number of data: ", l)

# Converting the dataframe into numpy array & removing the first column that contains the gene labels
X = []
X_masked = []
for i in range(len(df_list)):
  X.append(df_list[i].to_numpy())
  X[i] = X[i][:,2:]
  X_masked.append(np.array(X[i], dtype=float))

print(np.shape(X_masked))

# iterating the columns
phenotypes = []
for col in df_list[0].columns:
    phenotypes.append(col)

print(phenotypes)

phenotypes = phenotypes[1:]
print(phenotypes)
print(len(phenotypes))

genes = np.array(df_list[0].iloc[:, 0])
genes.tolist()
print(genes)

from sklearn.decomposition import PCA
import pandas as pd 
import seaborn as sns

pca = PCA(n_components=2, whiten=True) 
#fit the model to our data and extract the results

X_pca = pca.fit_transform(X_masked[0].T)
#create a dataframe from the dataset
df = pd.DataFrame(data = X_pca,
                 columns = ["Component 1",
                            "Component 2"])
#                            "Component 3"])

#                            "Component 4",
#                            "Component 5"])

print(df)

#plot the resulting data from two dimensions
g = sns.jointplot(data = df)

# for phenotype clustering
x = X_masked[0].T
# for genotype clustering
#x = X_masked[0]
print(np.shape(x))

x = np.expand_dims(x, axis=2) 
print(np.shape(x))

# Splitting test and train data 
#print(np.shape(x))

#x_train, x_test = train_test_split(x , random_state=104, test_size=0.01, shuffle=True)
x_train = x_test = x
print(np.shape(x_train[1]))
print(np.shape(x_train))
print(x_test[0])
print(np.shape(x_test))

"""Flatten the 5x8 test and train matrices into vectors of size 40."""

# Convert to numpy array
x_train = np.array(x_train)
x_test = np.array(x_test)

x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))
print(x_train.shape)
print(x_test.shape)

"""# Creating an autoencoder model
Starting with 3 fully-connected neural layers as encoder and as decoder.

Also, using a compression factor of 10 for now.

"""

# Setting the size of our encoded representations
encoding_dim =  np.shape(x)[2] * 100
print("Encoding dimensions :", encoding_dim) # -> compression of factor 20, assuming the input is 40 floats

# This is our input image
input_mat = keras.Input(shape=(x_train.shape[1],))
#encoded = layers.Dropout(0.1)(input_mat)
# "encoded" is the encoded representation of the input
encoded = layers.Dense(encoding_dim*5, activation='LeakyReLU', activity_regularizer = regularizers.l1(10e-5))(input_mat)
#encoded = layers.Dropout(0.1)(encoded)

encoded = layers.Dense(encoding_dim*2, activation='LeakyReLU')(encoded)
encoded = layers.Dense(encoding_dim, activation='LeakyReLU')(encoded)

# "decoded" is the lossy reconstruction of the input
decoded = layers.Dense(encoding_dim*2, activation='LeakyReLU')(encoded)
decoded = layers.Dense(encoding_dim*5, activation='LeakyReLU')(decoded)
decoded = layers.Dense(x_train.shape[1], activation='LeakyReLU')(decoded)


# Mapping the input to its reconstruction
autoencoder = keras.Model(input_mat, decoded)
autoencoder.summary()

"""Creating a separate encoder model:


"""

# Mapping an input to its encoded representation
encoder = keras.Model(input_mat, encoded)
encoder.summary()

"""As well as the decoder model:"""

# This is our encoded input with dim = 4
encoded_input = keras.Input(shape=(encoding_dim,))
# Retrieve the last 3 layer of the autoencoder model
decoder_layer1 = autoencoder.layers[-3]
decoder_layer2 = autoencoder.layers[-2]
decoder_layer3 = autoencoder.layers[-1]
# Create the decoder model
decoder = keras.Model(encoded_input, decoder_layer3(decoder_layer2(decoder_layer1(encoded_input))))
#decoder = keras.Model(encoded_input, decoder_layer3((encoded_input)))
decoder.summary()

"""# Train our autoencoder

Training to reconstruct the relationship matrices.
Configuring to use a per-element binary crossentropy loss, and the Adam optimizer:
"""

import tensorflow as tf
from keras import backend as K

# define the custom metrics
def rmse(y_true, y_pred):
  return K.sqrt(K.mean(K.square(y_pred - y_true), axis = -1))

#sgd = gradient_descent_v2.SGD(learning_rate=0.001, decay=1e-4, momentum=0.9, nesterov=True)
#adam = SGD(learning_rate=0.01, momentum=0.8)
#model.compile(optimizer='sgd', loss=tf.keras.losses.Huber())
autoencoder.compile(optimizer= 'adam', loss='mse', metrics=['mse','mae'])

"""Training for a set number of epochs


"""

num_epochs = 1000

from keras.callbacks import ModelCheckpoint, EarlyStopping 
mc = ModelCheckpoint('AE_model_RPE1_L100_pheno_combined_data.h5', monitor='val_mse', mode='min', verbose=2, save_best_only=True)
es = tf.keras.callbacks.EarlyStopping(
    monitor="val_mse",
    min_delta=0,
    patience=50,
    verbose=1,
    mode="min",
    baseline=None,
    restore_best_weights=True,
)

history = autoencoder.fit(x_train, x_train,
                epochs=num_epochs,
                batch_size=256,
                shuffle=True,
                validation_data=(x_test, x_test), callbacks=[mc, es])

loss = history.history['loss']
mse = history.history['mse']
val_loss = history.history['val_loss']
val_mse = history.history['val_mse']
#np.save('rpe1_normalized_AE_k=10_loss_geno', loss)
#np.save('rpe1_normalized_AE_k=10_val_loss_geno',val_loss)
#np.save('rpe1_normalized_AE_k=10_mse_geno', mse)
#np.save('rpe1_normalized_AE_k=10_val_mse_geno',val_mse)

import matplotlib.pyplot as plt

# list all data in history
print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['mse'])
plt.plot(history.history['val_mse'])
plt.title('Training the Autoencoder')
plt.ylabel('Mean squared error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.savefig("RPE1_100latent_mse_pheno_combined_data.jpg")
plt.show()

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

"""# Verifying the test data reconstruction

Checking the reconstructed inputs (from the decoder based on the encoded representations of the test data) and the real test data inputs.
"""

from keras.models import load_model, Model

# load the best model that was saved
saved_model = load_model('AE_model_K562_L100_pheno_combined_data.h5')

# evaluate the model
_, train_mse, train_mae = saved_model.evaluate(x_train, x_train, verbose=0)
_, test_mse, test_mae = saved_model.evaluate(x_test, x_test, verbose=0)
print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))


# Get the encoder part of the saved model with the learned weights
en_model = Model(saved_model.input, saved_model.layers[-4](saved_model.layers[-5](saved_model.layers[-6].output)))
de_input = keras.Input(shape=(encoding_dim,))
de_model = Model(de_input, autoencoder.layers[-1](autoencoder.layers[-2](autoencoder.layers[-3](de_input))))

encoded_mats = en_model.predict(x_test)
decoded_mats = de_model.predict(encoded_mats)

np.shape(encoded_mats)
np.save('AE_model_RPE1_L100_pheno_combined_data_encoded_mats', encoded_mats)

from matplotlib.pyplot import figure
import matplotlib.pyplot as plt

#plt.figure(figsize=(6, 6))
figure(figsize=(20,5))
plt.title('Encoded inputs')
plt.ylabel('Phenotype (gene_knockout)')
plt.xlabel('Encoding dimension')
plt.imshow(encoded_mats[0:10])
plt.colorbar()
plt.savefig("RPE1_100latent_encoded_mats_pheno_combined_data.jpg")

# Num of F instances to display
n = 1  
decoded_rounded = [None]*n
input_rounded = [None]*n

#input_rounded = np.array(input_rounded)

for i in range(n-1,n):
    # Display original F matrix
    input_rounded[i] = np.around(x_test[i], decimals=4, out=None)
    #print(input_rounded[i])
    print(input_rounded[i].reshape((np.array(x)).shape[1:]))
    
    # Display reconstructed matrix
    decoded_rounded[i] = np.around(decoded_mats[i], decimals=4, out=None)
    print(decoded_rounded[i].reshape((np.array(x)).shape[1:]))

"""# Testing different embeddings"""

from sklearn.decomposition import PCA
import pandas as pd 
import seaborn as sns

pca = PCA(n_components=2, whiten=True) 
#fit the model to our data and extract the results
X_pca = pca.fit_transform(encoded_mats)
#create a dataframe from the dataset
df = pd.DataFrame(data = X_pca,
                 columns = ["Component 1", 
                            "Component 2"])


print(df)

#plot the resulting data from two dimensions
g = sns.jointplot(data = df,
                 x = "Component 1",
                 y = "Component 2")
plt.savefig('RPE1_100latent_PCA_pheno_combined_data.png')

"""# t-SNE"""

from sklearn.manifold import TSNE
tsne = TSNE(verbose=1,n_iter=5000)
res = tsne.fit_transform(encoded_mats)

xmin = int(min(res[:,0]))
xmax = int(max(res[:,0]))
ymin = int(min(res[:,1]))
ymax = int(max(res[:,1]))
print(xmin, xmax, ymin, ymax)

from matplotlib.pyplot import figure
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(10,7))
scatter =plt.scatter(res[:,0],res[:,1], cmap='coolwarm', s=20)
#plt.savefig("RPE1_100latent_tSNE_pheno_combined_data.jpg")
plt.colorbar()
scatter.axes.get_xaxis().set_visible(False)
scatter.axes.get_yaxis().set_visible(False)

"""# Kmeans on the tSNE embedding"""

#Import required module
from sklearn.cluster import KMeans

def plot_clusters(df,labels):
  #Getting unique labels
  # print(df.shape)
  df['labels'] = labels 
  u_labels = np.unique(labels)
  
  #plotting the results:
  
  for i in u_labels:
      plt.scatter(df.values[labels == i , 0] , df.values[labels == i , 1])
  plt.legend()
  plt.colorbar()
  plt.show()

df1 = pd.DataFrame(data = res,
                 columns = ["Component 1",
                            "Component 2"])
print(np.shape(df1))
print(np.shape(x_train))

labels = []


#for rep in range(2,100):
for rep in range(13,14):
  kmeans = KMeans(n_clusters= rep, init='k-means++')
  label1 = kmeans.fit_predict(res)
  print(kmeans.cluster_centers_)
  plt.title("K = " + str(rep))
  plot_clusters(df1,label1)
  labels.append(label1)

import pandas as pd 
pd.DataFrame(res).to_csv("K562_res_points_100latent_combined_data.csv")

for x in res[:,0]:
  for y in res[:,1]:
    if x>-1 and x<1 and y<42 and y>40:
      print(x,y)

print(np.shape(np.array(labels).T))
phenotypes_arr = np.expand_dims(np.array(phenotypes).T, axis=1)
#genotypes_arr = np.expand_dims(np.array(df_list[0].gene_id).T, axis=1)
print(np.shape(phenotypes_arr))
#print(np.shape(genotypes_arr))

# Saving the labels from K-means clustering in a csv file
labels_named = np.hstack((phenotypes_arr, np.array(labels).T))
#labels_named = np.hstack((genotypes_arr, np.array(labels).T))
print(np.shape(labels_named))
print(labels_named[:,0:2])
np.save('RPE1_cluster_labels_100latent_K=2to99_pheno_combined_data', labels_named)

import pandas as pd 
pd.DataFrame(labels_named).to_csv("RPE1_cluster_labels_100latent_K=2to99_pheno_combined_data.csv")

"""# Agglomerative Clustering"""

from sklearn.cluster import AgglomerativeClustering
df1 = pd.DataFrame(data = res,
                 columns = ["Component 1",
                            "Component 2"])
labels2 = []


for rep in range(2,100):
  clustering = AgglomerativeClustering(n_clusters= rep).fit(res)
  label2 = clustering.labels_
  plt.title("K = " + str(rep))
  plot_clusters(df1,label2)
  labels2.append(label2)

#print(np.shape(np.array(phenotypes).T))
labels2_named = np.hstack((phenotypes_arr, np.array(labels2).T))
#labels2_named = np.hstack((genotypes_arr, np.array(labels2).T))
print(np.shape(labels2_named))
print(labels2_named[:,0:2])
np.save('RPE1_cluster_labels_100latent_agglo=2to99_pheno_combined_data', labels2_named)

import pandas as pd 
pd.DataFrame(labels2_named).to_csv("'RPE1_cluster_labels_100latent_agglo=2to99_pheno_combined_data.csv")

"""# Optimal k using elbow method"""

distortions = []
K = range(2,100)
for k in K:
    kmeanModel = KMeans(n_clusters=k, init='k-means++')
    kmeanModel.fit(res)
    distortions.append(kmeanModel.inertia_)

plt.figure(figsize=(16,8))
plt.plot(K, distortions, 'bx-')
plt.xlabel('k')
plt.xticks(np.arange(0,100, 5))
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal k')
plt.savefig("RPE1_100latent_k-elbow_pheno.jpg")
plt.show()

"""# Correlations in the data"""

from scipy import stats
corr, p_val = stats.spearmanr(x_train,axis=1)
print(p_val)
print(corr.shape)

import matplotlib.pyplot as plt

cor_mat = np.zeros((len(corr),len(corr[0])))
p = []
for i in range(len(corr)):
  pheno = []
  for j in range(len(corr[0])):
    if corr[i][j] >= 0.3:
      cor_mat[i][j] = 1
      if i != j:
        pheno.append(phenotypes[j])
  p.append(phenotypes[i])
  p.append(pheno)


plt.xlabel('phenotypes')
plt.ylabel('phenotypes')
plt.title('Spearmann correlation >= 0.3')
plt.imshow(cor_mat,interpolation='nearest', cmap='Greys_r')
plt.savefig("RPE1_100latent_corr_30_pheno_combined_data.png")
plt.colorbar()

#print(p)
ppp = []

for l in range(len(p)-1):
  if (l%2 == 0) & (p[l+1] != []):
    #print(p[l+1])
    pp = [p[l],p[l+1]]
    ppp.append(pp)

#print(len(ppp))
#print(ppp)

print(len(ppp[0][1]))

p4 = []
for j in range(len(ppp)):
  ppp[j][1].append(ppp[j][0])
  p4.append(ppp[j][1])

import pandas as pd
from numpy import asarray, savetxt
np.save('RPE1_k=100_pheno_corr_30_combined_data',np.array(p4))
pd.DataFrame(p4).to_csv("RPE1_k=100_pheno_corr_30_combined_data.csv")

labels2_named = np.load('RPE1_cluster_labels_10latent_K=2to99_pheno_combined_data.npy')
print(np.shape(labels2_named))

from numpy import load
labels2_named = np.load('RPE1_cluster_labels_100latent_K=2to99_pheno_combined_data.npy')
p4p = np.load('RPE1_k=100_pheno_corr_30_combined_data.npy', allow_pickle=True)

k = 5
grp_label = []
for a in range(len(p4p)):
  y = []
  for b in p4p[a]:
    for c in range(len(labels2_named)):
      if b == labels2_named[c,0]:
        y.append(labels2_named[c,k])
  grp_label.append(y)

#print(grp_label)

np.save('RPE1_k=100_pheno_corr_30_group_label_k=5_combined_data',np.array(grp_label))
pd.DataFrame(grp_label).to_csv("RPE1_k=100_pheno_corr_30_group_label_k=5_combined_data.csv")

p4

"""# HDB Scan embedding"""

!pip install hdbscan

import hdbscan

clusterer =  hdbscan.HDBSCAN(metric='precomputed', min_cluster_size=2, min_samples=1)
# clusterer.fit(mu)

from sklearn.metrics.pairwise import pairwise_distances
x = encoded_mats.astype("double")
distance_matrix =  pairwise_distances(x)
clusterer.fit(distance_matrix)

print(np.unique(clusterer.labels_,return_counts=True))

labels = clusterer.labels_
np.save('AE_model_K562_pheno_combined_data_k=100_hdb_labels_min_cluster_2', labels)

import pandas as pd

df = pd.DataFrame(labels)
df.to_csv('AE_model_K562_pheno_combined_data_k=100_hdb_labels_min_cluster_2.csv',index=False)